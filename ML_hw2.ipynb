{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_hw2",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abdullah-Al-Arafat/CAP-5610-Machine-Learning/blob/master/ML_hw2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "oeG5laAWNdNi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**CAP 5610: Machine Learning** \n",
        "Home work 2\n"
      ]
    },
    {
      "metadata": {
        "id": "uv5tiD3sNcGt",
        "colab_type": "code",
        "outputId": "8090e5c5-f977-4c8d-b2e2-dec3fcdb5c4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from keras.datasets import cifar10\n",
        "from keras import Sequential\n",
        "from keras.utils import np_utils\n",
        "from keras.optimizers import Adagrad, Adam\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Dense, Activation, Flatten, BatchNormalization\n",
        "from keras import regularizers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "zxr-0LoYWk22",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Preparation/preprocessing of train, test, and validation dataset "
      ]
    },
    {
      "metadata": {
        "id": "-tXx16k6Npl6",
        "colab_type": "code",
        "outputId": "76c2566f-0073-4651-aed9-256303004a9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "(train_images, train_labels),(test_images, test_labels) = cifar10.load_data()\n",
        "\n",
        "## Creating validation set from training dataset\n",
        "\n",
        "indices = np.random.permutation(len(train_images))\n",
        "\n",
        "val_indices = indices[0:10000]\n",
        "train_indices = indices[10000:]\n",
        "\n",
        "val_images, val_labels = train_images[val_indices], train_labels[val_indices]\n",
        "\n",
        "Train_images, Train_labels = train_images[train_indices], train_labels[train_indices]\n",
        "\n",
        "\n",
        "## Normalizing the Train, test and validation sets\n",
        "\n",
        "X_train = (Train_images/255).astype('float32')\n",
        "y_train = np_utils.to_categorical(Train_labels, 10)\n",
        "\n",
        "X_val = (val_images/255).astype('float32')\n",
        "y_val = np_utils.to_categorical(val_labels, 10)\n",
        "\n",
        "X_test = (test_images/255).astype('float32')\n",
        "y_test = np_utils.to_categorical(test_labels, 10)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 13s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wP0YCrbHWOnF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Model without Dropout layers and Data Augmentation** "
      ]
    },
    {
      "metadata": {
        "id": "GiEr6v1oOwSD",
        "colab_type": "code",
        "outputId": "daf4ca3d-9117-47b9-d245-12aa1d6dd512",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "cell_type": "code",
      "source": [
        "model1 = Sequential()\n",
        "model1.add(Conv2D(32, (3,3), strides = (1,1), padding='same', activation ='relu', input_shape =(32,32,3),kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(0.0001)))\n",
        "#model.add(BatchNormalization())\n",
        "#model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "\n",
        "model1.add(Conv2D(48, (3,3), strides = (1,1), padding='same', activation='relu', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(0.0001)))\n",
        "#model.add(BatchNormalization())\n",
        "#model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
        "\n",
        "model1.add(Conv2D(64, (3,3), strides= (1,1),padding='same', activation='relu', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(0.0001)))\n",
        "#model.add(BatchNormalization())\n",
        "model1.add(MaxPooling2D(pool_size=(2,2), padding= 'same'))\n",
        "#model1.add(Dropout(0.5))\n",
        " \n",
        "model1.add(Conv2D(128, (3,3), strides= (2,2), padding='same', activation = 'relu',kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(0.0001)))\n",
        "#model.add(BatchNormalization())\n",
        "model1.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
        "\n",
        "model1.add(Conv2D(256, (3,3), strides= (2,2), padding='same', activation = 'relu', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(0.0001)))\n",
        "#model.add(BatchNormalization())\n",
        "#model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model1.add(Flatten())\n",
        "#model.add(BatchNormalization())\n",
        "#model.add(Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.01))\n",
        "model1.add(Dense(128, activation='relu',  kernel_regularizer= regularizers.l2(0.0001)))\n",
        "model1.add(Dense(64, activation= 'relu'))\n",
        "model1.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model1.summary()\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 32, 32, 48)        13872     \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 32, 32, 64)        27712     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 2, 2, 256)         295168    \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               131200    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 551,610\n",
            "Trainable params: 551,610\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "I2kIBDnPO8JJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2053
        },
        "outputId": "1de34152-02fc-4f7f-ab27-3232677dea40"
      },
      "cell_type": "code",
      "source": [
        "# Initiate Adagrad/RMSprop optimizer\n",
        "model1.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adagrad',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "## Train the model and get the training and validation loss\n",
        "history = model1.fit(X_train, y_train, batch_size= 40, epochs= 120,\n",
        "                    verbose=1, validation_data=(X_val, y_val))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/120\n",
            "40000/40000 [==============================] - 36s 888us/step - loss: 1.6810 - acc: 0.4214 - val_loss: 1.3328 - val_acc: 0.5488\n",
            "Epoch 2/120\n",
            "40000/40000 [==============================] - 28s 700us/step - loss: 1.1302 - acc: 0.6289 - val_loss: 1.0462 - val_acc: 0.6605\n",
            "Epoch 3/120\n",
            "40000/40000 [==============================] - 28s 698us/step - loss: 0.9215 - acc: 0.7050 - val_loss: 1.0109 - val_acc: 0.6757\n",
            "Epoch 4/120\n",
            "40000/40000 [==============================] - 28s 710us/step - loss: 0.7846 - acc: 0.7535 - val_loss: 0.9052 - val_acc: 0.7071\n",
            "Epoch 5/120\n",
            "40000/40000 [==============================] - 30s 742us/step - loss: 0.6789 - acc: 0.7945 - val_loss: 0.8910 - val_acc: 0.7209\n",
            "Epoch 6/120\n",
            "40000/40000 [==============================] - 30s 746us/step - loss: 0.5925 - acc: 0.8263 - val_loss: 0.9016 - val_acc: 0.7284\n",
            "Epoch 7/120\n",
            "40000/40000 [==============================] - 29s 733us/step - loss: 0.5130 - acc: 0.8556 - val_loss: 0.9277 - val_acc: 0.7259\n",
            "Epoch 8/120\n",
            "40000/40000 [==============================] - 28s 709us/step - loss: 0.4376 - acc: 0.8821 - val_loss: 0.9241 - val_acc: 0.7388\n",
            "Epoch 9/120\n",
            "40000/40000 [==============================] - 28s 695us/step - loss: 0.3725 - acc: 0.9079 - val_loss: 0.9627 - val_acc: 0.7349\n",
            "Epoch 10/120\n",
            "40000/40000 [==============================] - 27s 686us/step - loss: 0.3140 - acc: 0.9300 - val_loss: 1.0807 - val_acc: 0.7306\n",
            "Epoch 11/120\n",
            "40000/40000 [==============================] - 29s 716us/step - loss: 0.2593 - acc: 0.9487 - val_loss: 1.1125 - val_acc: 0.7297\n",
            "Epoch 12/120\n",
            "40000/40000 [==============================] - 29s 716us/step - loss: 0.2169 - acc: 0.9644 - val_loss: 1.2081 - val_acc: 0.7291\n",
            "Epoch 13/120\n",
            "40000/40000 [==============================] - 28s 705us/step - loss: 0.1816 - acc: 0.9773 - val_loss: 1.2942 - val_acc: 0.7317\n",
            "Epoch 14/120\n",
            "40000/40000 [==============================] - 27s 673us/step - loss: 0.1538 - acc: 0.9876 - val_loss: 1.3760 - val_acc: 0.7313\n",
            "Epoch 15/120\n",
            "40000/40000 [==============================] - 27s 663us/step - loss: 0.1338 - acc: 0.9937 - val_loss: 1.4699 - val_acc: 0.7332\n",
            "Epoch 16/120\n",
            "40000/40000 [==============================] - 28s 689us/step - loss: 0.1192 - acc: 0.9978 - val_loss: 1.5678 - val_acc: 0.7323\n",
            "Epoch 17/120\n",
            "40000/40000 [==============================] - 30s 744us/step - loss: 0.1103 - acc: 0.9993 - val_loss: 1.6364 - val_acc: 0.7309\n",
            "Epoch 18/120\n",
            "40000/40000 [==============================] - 30s 740us/step - loss: 0.1053 - acc: 0.9998 - val_loss: 1.7143 - val_acc: 0.7349\n",
            "Epoch 19/120\n",
            "40000/40000 [==============================] - 27s 684us/step - loss: 0.1025 - acc: 1.0000 - val_loss: 1.7636 - val_acc: 0.7315\n",
            "Epoch 20/120\n",
            "40000/40000 [==============================] - 29s 718us/step - loss: 0.1009 - acc: 1.0000 - val_loss: 1.8041 - val_acc: 0.7333\n",
            "Epoch 21/120\n",
            "40000/40000 [==============================] - 27s 686us/step - loss: 0.0996 - acc: 1.0000 - val_loss: 1.8315 - val_acc: 0.7335\n",
            "Epoch 22/120\n",
            "40000/40000 [==============================] - 28s 691us/step - loss: 0.0986 - acc: 1.0000 - val_loss: 1.8576 - val_acc: 0.7313\n",
            "Epoch 23/120\n",
            "40000/40000 [==============================] - 26s 641us/step - loss: 0.0977 - acc: 1.0000 - val_loss: 1.8848 - val_acc: 0.7314\n",
            "Epoch 24/120\n",
            "40000/40000 [==============================] - 25s 635us/step - loss: 0.0969 - acc: 1.0000 - val_loss: 1.9018 - val_acc: 0.7328\n",
            "Epoch 25/120\n",
            "40000/40000 [==============================] - 25s 634us/step - loss: 0.0961 - acc: 1.0000 - val_loss: 1.9131 - val_acc: 0.7324\n",
            "Epoch 26/120\n",
            "40000/40000 [==============================] - 25s 632us/step - loss: 0.0954 - acc: 1.0000 - val_loss: 1.9293 - val_acc: 0.7311\n",
            "Epoch 27/120\n",
            "40000/40000 [==============================] - 25s 631us/step - loss: 0.0947 - acc: 1.0000 - val_loss: 1.9412 - val_acc: 0.7313\n",
            "Epoch 28/120\n",
            "40000/40000 [==============================] - 25s 630us/step - loss: 0.0941 - acc: 1.0000 - val_loss: 1.9522 - val_acc: 0.7333\n",
            "Epoch 29/120\n",
            "40000/40000 [==============================] - 25s 622us/step - loss: 0.0934 - acc: 1.0000 - val_loss: 1.9559 - val_acc: 0.7304\n",
            "Epoch 30/120\n",
            "40000/40000 [==============================] - 25s 618us/step - loss: 0.0928 - acc: 1.0000 - val_loss: 1.9675 - val_acc: 0.7314\n",
            "Epoch 31/120\n",
            "39880/40000 [============================>.] - ETA: 0s - loss: 0.0922 - acc: 1.0000"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-c49834c94cac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m## Train the model and get the training and validation loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m history = model1.fit(X_train, y_train, batch_size= 40, epochs= 120,\n\u001b[0;32m----> 7\u001b[0;31m                     verbose=1, validation_data=(X_val, y_val))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    210\u001b[0m                         val_outs = test_loop(model, val_f, val_ins,\n\u001b[1;32m    211\u001b[0m                                              \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m                                              verbose=0)\n\u001b[0m\u001b[1;32m    213\u001b[0m                         \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                         \u001b[0;31m# Same labels assumed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "MGefs0kuPJIU",
        "colab_type": "code",
        "outputId": "d506a528-b577-467c-dfbd-35ddedac851b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "## Score trained model.\n",
        "scores = model1.evaluate(X_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 2s 243us/step\n",
            "Test loss: 1.9841494254112244\n",
            "Test accuracy: 0.7339\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YMREmegVbnAk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "model1 is clearly overfitting the training dataset, althought I have added L1 and L2 regularization in the Conv layers and FC layers.\n",
        "\n",
        "**Model2** with dropout layers"
      ]
    },
    {
      "metadata": {
        "id": "hUISC_wvPT0K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "4317173f-240c-42bc-bdac-71e6d3321c31"
      },
      "cell_type": "code",
      "source": [
        "model2 = Sequential()\n",
        "model2.add(Conv2D(32, (3,3), strides = (1,1), padding='same', activation ='relu', input_shape =(32,32,3),kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(0.0001)))\n",
        "model2.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "model2.add(Conv2D(48, (3,3), strides = (1,1), padding='same', activation='relu', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(0.0001)))\n",
        "model2.add(Dropout(0.25))\n",
        "\n",
        "model2.add(Conv2D(64, (3,3), strides= (1,1),padding='same', activation='relu', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(0.0001)))\n",
        "model2.add(MaxPooling2D(pool_size=(2,2), padding= 'same'))\n",
        "model1.add(Dropout(0.5))\n",
        " \n",
        "model2.add(Conv2D(128, (3,3), strides= (2,2), padding='same', activation = 'relu',kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(0.0001)))\n",
        "model2.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
        "model2.add(Dropout(0.25))\n",
        "\n",
        "model2.add(Conv2D(256, (3,3), strides= (2,2), padding='same', activation = 'relu', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(0.0001)))\n",
        "model2.add(Dropout(0.1))\n",
        "\n",
        "model2.add(Flatten())\n",
        "model2.add(Dense(128, activation='relu',  kernel_regularizer= regularizers.l2(0.0001)))\n",
        "model2.add(Dense(64, activation= 'relu'))\n",
        "model2.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model2.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 32, 32, 48)        13872     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 32, 32, 48)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 32, 32, 64)        27712     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 2, 2, 256)         295168    \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 128)               131200    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 551,610\n",
            "Trainable params: 551,610\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wQ02ECWbepQ1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1054
        },
        "outputId": "a1b7d660-151c-4acc-f714-36b679431145"
      },
      "cell_type": "code",
      "source": [
        "# Initiate Adagrad/RMSprop optimizer\n",
        "model2.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adagrad',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "## Train the model and get the training and validation loss\n",
        "history = model2.fit(X_train, y_train, batch_size= 40, epochs= 30,\n",
        "                    verbose=1, validation_data=(X_val, y_val))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/30\n",
            "40000/40000 [==============================] - 34s 843us/step - loss: 1.7304 - acc: 0.3987 - val_loss: 1.3951 - val_acc: 0.5277\n",
            "Epoch 2/30\n",
            "40000/40000 [==============================] - 34s 847us/step - loss: 1.3044 - acc: 0.5622 - val_loss: 1.1723 - val_acc: 0.6130\n",
            "Epoch 3/30\n",
            "40000/40000 [==============================] - 33s 834us/step - loss: 1.1258 - acc: 0.6323 - val_loss: 1.0529 - val_acc: 0.6586\n",
            "Epoch 4/30\n",
            "40000/40000 [==============================] - 32s 800us/step - loss: 1.0166 - acc: 0.6769 - val_loss: 1.0235 - val_acc: 0.6735\n",
            "Epoch 5/30\n",
            "40000/40000 [==============================] - 33s 836us/step - loss: 0.9403 - acc: 0.7009 - val_loss: 0.9532 - val_acc: 0.7014\n",
            "Epoch 6/30\n",
            "40000/40000 [==============================] - 34s 853us/step - loss: 0.8791 - acc: 0.7241 - val_loss: 0.9150 - val_acc: 0.7137\n",
            "Epoch 7/30\n",
            "40000/40000 [==============================] - 33s 826us/step - loss: 0.8319 - acc: 0.7417 - val_loss: 0.9171 - val_acc: 0.7165\n",
            "Epoch 8/30\n",
            "40000/40000 [==============================] - 33s 834us/step - loss: 0.7906 - acc: 0.7550 - val_loss: 0.8813 - val_acc: 0.7280\n",
            "Epoch 9/30\n",
            "40000/40000 [==============================] - 34s 841us/step - loss: 0.7506 - acc: 0.7710 - val_loss: 0.8486 - val_acc: 0.7396\n",
            "Epoch 10/30\n",
            "40000/40000 [==============================] - 33s 826us/step - loss: 0.7174 - acc: 0.7847 - val_loss: 0.8366 - val_acc: 0.7442\n",
            "Epoch 11/30\n",
            "40000/40000 [==============================] - 34s 846us/step - loss: 0.6885 - acc: 0.7945 - val_loss: 0.8679 - val_acc: 0.7344\n",
            "Epoch 12/30\n",
            "40000/40000 [==============================] - 33s 817us/step - loss: 0.6608 - acc: 0.8024 - val_loss: 0.8262 - val_acc: 0.7475\n",
            "Epoch 13/30\n",
            "40000/40000 [==============================] - 34s 840us/step - loss: 0.6347 - acc: 0.8132 - val_loss: 0.8274 - val_acc: 0.7493\n",
            "Epoch 14/30\n",
            "40000/40000 [==============================] - 32s 799us/step - loss: 0.6074 - acc: 0.8227 - val_loss: 0.8130 - val_acc: 0.7540\n",
            "Epoch 15/30\n",
            "40000/40000 [==============================] - 32s 810us/step - loss: 0.5884 - acc: 0.8291 - val_loss: 0.8133 - val_acc: 0.7548\n",
            "Epoch 16/30\n",
            "40000/40000 [==============================] - 34s 839us/step - loss: 0.5672 - acc: 0.8365 - val_loss: 0.8343 - val_acc: 0.7504\n",
            "Epoch 17/30\n",
            "40000/40000 [==============================] - 33s 835us/step - loss: 0.5470 - acc: 0.8444 - val_loss: 0.8194 - val_acc: 0.7616\n",
            "Epoch 18/30\n",
            "40000/40000 [==============================] - 34s 845us/step - loss: 0.5258 - acc: 0.8522 - val_loss: 0.8216 - val_acc: 0.7562\n",
            "Epoch 19/30\n",
            "40000/40000 [==============================] - 33s 834us/step - loss: 0.5122 - acc: 0.8572 - val_loss: 0.8236 - val_acc: 0.7606\n",
            "Epoch 20/30\n",
            "40000/40000 [==============================] - 33s 831us/step - loss: 0.4932 - acc: 0.8620 - val_loss: 0.8193 - val_acc: 0.7654\n",
            "Epoch 21/30\n",
            "40000/40000 [==============================] - 33s 829us/step - loss: 0.4807 - acc: 0.8683 - val_loss: 0.8268 - val_acc: 0.7677\n",
            "Epoch 22/30\n",
            "40000/40000 [==============================] - 33s 821us/step - loss: 0.4664 - acc: 0.8719 - val_loss: 0.8278 - val_acc: 0.7680\n",
            "Epoch 23/30\n",
            "40000/40000 [==============================] - 33s 816us/step - loss: 0.4488 - acc: 0.8793 - val_loss: 0.8369 - val_acc: 0.7659\n",
            "Epoch 24/30\n",
            "40000/40000 [==============================] - 33s 834us/step - loss: 0.4396 - acc: 0.8824 - val_loss: 0.8473 - val_acc: 0.7615\n",
            "Epoch 25/30\n",
            "40000/40000 [==============================] - 35s 863us/step - loss: 0.4248 - acc: 0.8888 - val_loss: 0.8629 - val_acc: 0.7644\n",
            "Epoch 26/30\n",
            "40000/40000 [==============================] - 33s 825us/step - loss: 0.4172 - acc: 0.8921 - val_loss: 0.8493 - val_acc: 0.7670\n",
            "Epoch 27/30\n",
            "40000/40000 [==============================] - 35s 868us/step - loss: 0.4068 - acc: 0.8957 - val_loss: 0.8608 - val_acc: 0.7638\n",
            "Epoch 28/30\n",
            "40000/40000 [==============================] - 34s 842us/step - loss: 0.3940 - acc: 0.8997 - val_loss: 0.8740 - val_acc: 0.7651\n",
            "Epoch 29/30\n",
            "40000/40000 [==============================] - 35s 887us/step - loss: 0.3829 - acc: 0.9036 - val_loss: 0.8678 - val_acc: 0.7686\n",
            "Epoch 30/30\n",
            "40000/40000 [==============================] - 34s 860us/step - loss: 0.3775 - acc: 0.9068 - val_loss: 0.8639 - val_acc: 0.7710\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g6gzMaNgeehk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9b721c0e-01c8-4c5e-a2d6-760cb0cfd6dd"
      },
      "cell_type": "code",
      "source": [
        "## Score trained model.\n",
        "scores = model2.evaluate(X_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 2s 230us/step\n",
            "Test loss: 0.8814269060134887\n",
            "Test accuracy: 0.7711\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ReGd9DUMhjHn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Model2** significantly reduce the varince, now I'll introduce data augmentation in the model3"
      ]
    },
    {
      "metadata": {
        "id": "GzCqQ3cUhgZ3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#data augmentation\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "        rotation_range=60,\n",
        "        width_shift_range=0.15,\n",
        "        height_shift_range=0.15,\n",
        "        shear_range=0.15,\n",
        "        zoom_range=0.15,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1UXOdyFZjTnw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model3 = Sequential()\n",
        "model3.add(Conv2D(32, (3,3), strides = (1,1), padding='same', activation ='relu', input_shape =(32,32,3),kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(0.0001)))\n",
        "\n",
        "\n",
        "model3.add(Conv2D(48, (3,3), strides = (1,1), padding='same', activation='relu', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(0.0001)))\n",
        "\n",
        "model3.add(Conv2D(64, (3,3), strides= (1,1),padding='same', activation='relu', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(0.0001)))\n",
        "model3.add(MaxPooling2D(pool_size=(2,2), padding= 'same'))\n",
        " \n",
        "model3.add(Conv2D(128, (3,3), strides= (2,2), padding='same', activation = 'relu',kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(0.0001)))\n",
        "model3.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
        "\n",
        "model3.add(Conv2D(256, (3,3), strides= (2,2), padding='same', activation = 'relu', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(0.0001)))\n",
        "\n",
        "model3.add(Flatten())\n",
        "model3.add(Dense(128, activation='relu',  kernel_regularizer= regularizers.l2(0.0001)))\n",
        "model3.add(Dense(64, activation= 'relu'))\n",
        "model3.add(Dense(10, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fi6eewZ1j3dJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4114
        },
        "outputId": "e35ee111-c502-45f3-d701-64e57a19803e"
      },
      "cell_type": "code",
      "source": [
        "model3.compile(loss='categorical_crossentropy', \n",
        "               optimizer='adagrad', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "## Train the model and get the training and validation loss\n",
        "model3.fit_generator(datagen.flow(X_train, y_train, batch_size=40),\n",
        "                    steps_per_epoch=train_images.shape[0]/ 40,epochs= 120,\n",
        "                    verbose=1,validation_data=(X_val,y_val))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/120\n",
            "1250/1250 [==============================] - 57s 45ms/step - loss: 1.8879 - acc: 0.3326 - val_loss: 1.6427 - val_acc: 0.4284\n",
            "Epoch 2/120\n",
            "1250/1250 [==============================] - 56s 45ms/step - loss: 1.6012 - acc: 0.4417 - val_loss: 1.5006 - val_acc: 0.4749\n",
            "Epoch 3/120\n",
            "1250/1250 [==============================] - 56s 45ms/step - loss: 1.4708 - acc: 0.4979 - val_loss: 1.3240 - val_acc: 0.5484\n",
            "Epoch 4/120\n",
            "1250/1250 [==============================] - 59s 47ms/step - loss: 1.3746 - acc: 0.5299 - val_loss: 1.3202 - val_acc: 0.5565\n",
            "Epoch 5/120\n",
            "1250/1250 [==============================] - 61s 49ms/step - loss: 1.3076 - acc: 0.5555 - val_loss: 1.1718 - val_acc: 0.6089\n",
            "Epoch 6/120\n",
            "1250/1250 [==============================] - 58s 47ms/step - loss: 1.2557 - acc: 0.5780 - val_loss: 1.1702 - val_acc: 0.6122\n",
            "Epoch 7/120\n",
            "1250/1250 [==============================] - 60s 48ms/step - loss: 1.2165 - acc: 0.5891 - val_loss: 1.1410 - val_acc: 0.6256\n",
            "Epoch 8/120\n",
            "1250/1250 [==============================] - 60s 48ms/step - loss: 1.1788 - acc: 0.6089 - val_loss: 1.1307 - val_acc: 0.6320\n",
            "Epoch 9/120\n",
            "1250/1250 [==============================] - 57s 45ms/step - loss: 1.1579 - acc: 0.6142 - val_loss: 1.0803 - val_acc: 0.6488\n",
            "Epoch 10/120\n",
            "1250/1250 [==============================] - 58s 47ms/step - loss: 1.1219 - acc: 0.6280 - val_loss: 1.0895 - val_acc: 0.6472\n",
            "Epoch 11/120\n",
            "1250/1250 [==============================] - 58s 47ms/step - loss: 1.1029 - acc: 0.6349 - val_loss: 1.1088 - val_acc: 0.6440\n",
            "Epoch 12/120\n",
            "1250/1250 [==============================] - 61s 49ms/step - loss: 1.0868 - acc: 0.6425 - val_loss: 1.0635 - val_acc: 0.6576\n",
            "Epoch 13/120\n",
            "1250/1250 [==============================] - 59s 47ms/step - loss: 1.0678 - acc: 0.6464 - val_loss: 1.0514 - val_acc: 0.6642\n",
            "Epoch 14/120\n",
            "1250/1250 [==============================] - 59s 47ms/step - loss: 1.0520 - acc: 0.6517 - val_loss: 1.0152 - val_acc: 0.6780\n",
            "Epoch 15/120\n",
            "1250/1250 [==============================] - 59s 47ms/step - loss: 1.0463 - acc: 0.6566 - val_loss: 1.1080 - val_acc: 0.6443\n",
            "Epoch 16/120\n",
            "1250/1250 [==============================] - 58s 47ms/step - loss: 1.0259 - acc: 0.6633 - val_loss: 1.0199 - val_acc: 0.6713\n",
            "Epoch 17/120\n",
            "1250/1250 [==============================] - 59s 47ms/step - loss: 1.0161 - acc: 0.6666 - val_loss: 0.9647 - val_acc: 0.6893\n",
            "Epoch 18/120\n",
            "1250/1250 [==============================] - 58s 46ms/step - loss: 1.0020 - acc: 0.6725 - val_loss: 0.9915 - val_acc: 0.6824\n",
            "Epoch 19/120\n",
            "1250/1250 [==============================] - 59s 48ms/step - loss: 0.9900 - acc: 0.6805 - val_loss: 1.0280 - val_acc: 0.6732\n",
            "Epoch 20/120\n",
            "1250/1250 [==============================] - 57s 45ms/step - loss: 0.9879 - acc: 0.6786 - val_loss: 0.9805 - val_acc: 0.6854\n",
            "Epoch 21/120\n",
            "1250/1250 [==============================] - 56s 45ms/step - loss: 0.9721 - acc: 0.6836 - val_loss: 1.0107 - val_acc: 0.6755\n",
            "Epoch 22/120\n",
            "1250/1250 [==============================] - 61s 49ms/step - loss: 0.9683 - acc: 0.6827 - val_loss: 1.0478 - val_acc: 0.6710\n",
            "Epoch 23/120\n",
            "1250/1250 [==============================] - 60s 48ms/step - loss: 0.9538 - acc: 0.6925 - val_loss: 0.9807 - val_acc: 0.6872\n",
            "Epoch 24/120\n",
            "1250/1250 [==============================] - 58s 46ms/step - loss: 0.9494 - acc: 0.6917 - val_loss: 0.9664 - val_acc: 0.6947\n",
            "Epoch 25/120\n",
            "1250/1250 [==============================] - 67s 54ms/step - loss: 0.9467 - acc: 0.6948 - val_loss: 0.9413 - val_acc: 0.7003\n",
            "Epoch 26/120\n",
            "1250/1250 [==============================] - 60s 48ms/step - loss: 0.9300 - acc: 0.7005 - val_loss: 0.9840 - val_acc: 0.6897\n",
            "Epoch 27/120\n",
            "1250/1250 [==============================] - 61s 49ms/step - loss: 0.9245 - acc: 0.7008 - val_loss: 1.0078 - val_acc: 0.6785\n",
            "Epoch 28/120\n",
            "1250/1250 [==============================] - 59s 47ms/step - loss: 0.9188 - acc: 0.7048 - val_loss: 0.8988 - val_acc: 0.7129\n",
            "Epoch 29/120\n",
            "1250/1250 [==============================] - 59s 47ms/step - loss: 0.9128 - acc: 0.7066 - val_loss: 0.9205 - val_acc: 0.7075\n",
            "Epoch 30/120\n",
            "1250/1250 [==============================] - 59s 47ms/step - loss: 0.9080 - acc: 0.7075 - val_loss: 0.9780 - val_acc: 0.6909\n",
            "Epoch 31/120\n",
            "1250/1250 [==============================] - 59s 47ms/step - loss: 0.9041 - acc: 0.7087 - val_loss: 0.9701 - val_acc: 0.6977\n",
            "Epoch 32/120\n",
            "1250/1250 [==============================] - 58s 46ms/step - loss: 0.8970 - acc: 0.7122 - val_loss: 0.9435 - val_acc: 0.7010\n",
            "Epoch 33/120\n",
            "1250/1250 [==============================] - 58s 46ms/step - loss: 0.8872 - acc: 0.7175 - val_loss: 0.9613 - val_acc: 0.6999\n",
            "Epoch 34/120\n",
            "1250/1250 [==============================] - 57s 46ms/step - loss: 0.8921 - acc: 0.7136 - val_loss: 0.9240 - val_acc: 0.7044\n",
            "Epoch 35/120\n",
            "1250/1250 [==============================] - 59s 47ms/step - loss: 0.8811 - acc: 0.7180 - val_loss: 0.9005 - val_acc: 0.7191\n",
            "Epoch 36/120\n",
            "1250/1250 [==============================] - 62s 49ms/step - loss: 0.8787 - acc: 0.7177 - val_loss: 0.9210 - val_acc: 0.7100\n",
            "Epoch 37/120\n",
            "1250/1250 [==============================] - 61s 49ms/step - loss: 0.8739 - acc: 0.7196 - val_loss: 0.9554 - val_acc: 0.7018\n",
            "Epoch 38/120\n",
            "1250/1250 [==============================] - 59s 48ms/step - loss: 0.8698 - acc: 0.7231 - val_loss: 0.9071 - val_acc: 0.7153\n",
            "Epoch 39/120\n",
            "1250/1250 [==============================] - 61s 49ms/step - loss: 0.8639 - acc: 0.7234 - val_loss: 0.9078 - val_acc: 0.7153\n",
            "Epoch 40/120\n",
            "1250/1250 [==============================] - 59s 47ms/step - loss: 0.8560 - acc: 0.7261 - val_loss: 0.9221 - val_acc: 0.7115\n",
            "Epoch 41/120\n",
            "1250/1250 [==============================] - 61s 49ms/step - loss: 0.8559 - acc: 0.7277 - val_loss: 0.9450 - val_acc: 0.7074\n",
            "Epoch 42/120\n",
            "1250/1250 [==============================] - 59s 47ms/step - loss: 0.8532 - acc: 0.7286 - val_loss: 0.9468 - val_acc: 0.7087\n",
            "Epoch 43/120\n",
            "1250/1250 [==============================] - 61s 49ms/step - loss: 0.8429 - acc: 0.7318 - val_loss: 0.8879 - val_acc: 0.7214\n",
            "Epoch 44/120\n",
            "1250/1250 [==============================] - 62s 50ms/step - loss: 0.8464 - acc: 0.7294 - val_loss: 0.8797 - val_acc: 0.7222\n",
            "Epoch 45/120\n",
            "1250/1250 [==============================] - 61s 49ms/step - loss: 0.8400 - acc: 0.7324 - val_loss: 0.8873 - val_acc: 0.7205\n",
            "Epoch 46/120\n",
            "1250/1250 [==============================] - 63s 50ms/step - loss: 0.8384 - acc: 0.7319 - val_loss: 0.8979 - val_acc: 0.7203\n",
            "Epoch 47/120\n",
            "1250/1250 [==============================] - 61s 49ms/step - loss: 0.8336 - acc: 0.7368 - val_loss: 0.9273 - val_acc: 0.7151\n",
            "Epoch 48/120\n",
            "1250/1250 [==============================] - 57s 45ms/step - loss: 0.8252 - acc: 0.7381 - val_loss: 0.8640 - val_acc: 0.7328\n",
            "Epoch 49/120\n",
            "1250/1250 [==============================] - 59s 47ms/step - loss: 0.8252 - acc: 0.7395 - val_loss: 0.8789 - val_acc: 0.7267\n",
            "Epoch 50/120\n",
            "1250/1250 [==============================] - 58s 46ms/step - loss: 0.8229 - acc: 0.7398 - val_loss: 0.8869 - val_acc: 0.7276\n",
            "Epoch 51/120\n",
            "1250/1250 [==============================] - 57s 46ms/step - loss: 0.8194 - acc: 0.7402 - val_loss: 0.8765 - val_acc: 0.7302\n",
            "Epoch 52/120\n",
            "1250/1250 [==============================] - 58s 47ms/step - loss: 0.8142 - acc: 0.7427 - val_loss: 0.8862 - val_acc: 0.7282\n",
            "Epoch 53/120\n",
            "1250/1250 [==============================] - 54s 43ms/step - loss: 0.8126 - acc: 0.7425 - val_loss: 0.9014 - val_acc: 0.7241\n",
            "Epoch 54/120\n",
            "1250/1250 [==============================] - 52s 41ms/step - loss: 0.8125 - acc: 0.7436 - val_loss: 0.8526 - val_acc: 0.7354\n",
            "Epoch 55/120\n",
            "1250/1250 [==============================] - 58s 47ms/step - loss: 0.8074 - acc: 0.7450 - val_loss: 0.8634 - val_acc: 0.7301\n",
            "Epoch 56/120\n",
            "1250/1250 [==============================] - 56s 45ms/step - loss: 0.8092 - acc: 0.7436 - val_loss: 0.8721 - val_acc: 0.7285\n",
            "Epoch 57/120\n",
            "1250/1250 [==============================] - 58s 46ms/step - loss: 0.8031 - acc: 0.7469 - val_loss: 0.8629 - val_acc: 0.7330\n",
            "Epoch 58/120\n",
            "1250/1250 [==============================] - 59s 47ms/step - loss: 0.7957 - acc: 0.7486 - val_loss: 0.8611 - val_acc: 0.7320\n",
            "Epoch 59/120\n",
            "1250/1250 [==============================] - 59s 47ms/step - loss: 0.7965 - acc: 0.7481 - val_loss: 0.8373 - val_acc: 0.7402\n",
            "Epoch 60/120\n",
            "1250/1250 [==============================] - 58s 46ms/step - loss: 0.7988 - acc: 0.7479 - val_loss: 0.9152 - val_acc: 0.7190\n",
            "Epoch 61/120\n",
            "1250/1250 [==============================] - 56s 45ms/step - loss: 0.7904 - acc: 0.7534 - val_loss: 0.8414 - val_acc: 0.7401\n",
            "Epoch 62/120\n",
            "1250/1250 [==============================] - 58s 46ms/step - loss: 0.7886 - acc: 0.7537 - val_loss: 0.8627 - val_acc: 0.7349\n",
            "Epoch 63/120\n",
            "1250/1250 [==============================] - 57s 45ms/step - loss: 0.7921 - acc: 0.7509 - val_loss: 0.8770 - val_acc: 0.7309\n",
            "Epoch 64/120\n",
            "1250/1250 [==============================] - 58s 47ms/step - loss: 0.7881 - acc: 0.7524 - val_loss: 0.8381 - val_acc: 0.7442\n",
            "Epoch 65/120\n",
            "1250/1250 [==============================] - 58s 46ms/step - loss: 0.7804 - acc: 0.7549 - val_loss: 0.8006 - val_acc: 0.7546\n",
            "Epoch 66/120\n",
            "1250/1250 [==============================] - 56s 45ms/step - loss: 0.7813 - acc: 0.7546 - val_loss: 0.8759 - val_acc: 0.7323\n",
            "Epoch 67/120\n",
            "1250/1250 [==============================] - 57s 46ms/step - loss: 0.7738 - acc: 0.7590 - val_loss: 0.8943 - val_acc: 0.7293\n",
            "Epoch 68/120\n",
            "1250/1250 [==============================] - 58s 47ms/step - loss: 0.7704 - acc: 0.7571 - val_loss: 0.8615 - val_acc: 0.7354\n",
            "Epoch 69/120\n",
            "1250/1250 [==============================] - 54s 43ms/step - loss: 0.7673 - acc: 0.7599 - val_loss: 0.8733 - val_acc: 0.7350\n",
            "Epoch 70/120\n",
            "1250/1250 [==============================] - 57s 46ms/step - loss: 0.7724 - acc: 0.7572 - val_loss: 0.8212 - val_acc: 0.7474\n",
            "Epoch 71/120\n",
            "1250/1250 [==============================] - 54s 43ms/step - loss: 0.7640 - acc: 0.7583 - val_loss: 0.8666 - val_acc: 0.7368\n",
            "Epoch 72/120\n",
            "1250/1250 [==============================] - 58s 46ms/step - loss: 0.7653 - acc: 0.7607 - val_loss: 0.8297 - val_acc: 0.7474\n",
            "Epoch 73/120\n",
            "1250/1250 [==============================] - 58s 46ms/step - loss: 0.7585 - acc: 0.7623 - val_loss: 0.8521 - val_acc: 0.7410\n",
            "Epoch 74/120\n",
            "1250/1250 [==============================] - 60s 48ms/step - loss: 0.7563 - acc: 0.7616 - val_loss: 0.8592 - val_acc: 0.7385\n",
            "Epoch 75/120\n",
            "1250/1250 [==============================] - 57s 46ms/step - loss: 0.7608 - acc: 0.7615 - val_loss: 0.8407 - val_acc: 0.7412\n",
            "Epoch 76/120\n",
            "1250/1250 [==============================] - 58s 46ms/step - loss: 0.7550 - acc: 0.7625 - val_loss: 0.8599 - val_acc: 0.7409\n",
            "Epoch 77/120\n",
            "1250/1250 [==============================] - 58s 46ms/step - loss: 0.7560 - acc: 0.7638 - val_loss: 0.8259 - val_acc: 0.7471\n",
            "Epoch 78/120\n",
            "1250/1250 [==============================] - 57s 45ms/step - loss: 0.7568 - acc: 0.7620 - val_loss: 0.8368 - val_acc: 0.7465\n",
            "Epoch 79/120\n",
            "1250/1250 [==============================] - 56s 45ms/step - loss: 0.7523 - acc: 0.7650 - val_loss: 0.8435 - val_acc: 0.7491\n",
            "Epoch 80/120\n",
            "1250/1250 [==============================] - 59s 47ms/step - loss: 0.7528 - acc: 0.7638 - val_loss: 0.8313 - val_acc: 0.7452\n",
            "Epoch 81/120\n",
            "1250/1250 [==============================] - 61s 49ms/step - loss: 0.7443 - acc: 0.7664 - val_loss: 0.8644 - val_acc: 0.7372\n",
            "Epoch 82/120\n",
            "1250/1250 [==============================] - 61s 49ms/step - loss: 0.7481 - acc: 0.7659 - val_loss: 0.7997 - val_acc: 0.7620\n",
            "Epoch 83/120\n",
            "1250/1250 [==============================] - 60s 48ms/step - loss: 0.7442 - acc: 0.7673 - val_loss: 0.8416 - val_acc: 0.7473\n",
            "Epoch 84/120\n",
            "1250/1250 [==============================] - 57s 45ms/step - loss: 0.7420 - acc: 0.7683 - val_loss: 0.8383 - val_acc: 0.7445\n",
            "Epoch 85/120\n",
            "1250/1250 [==============================] - 58s 46ms/step - loss: 0.7384 - acc: 0.7692 - val_loss: 0.8277 - val_acc: 0.7498\n",
            "Epoch 86/120\n",
            "1250/1250 [==============================] - 59s 47ms/step - loss: 0.7372 - acc: 0.7688 - val_loss: 0.8422 - val_acc: 0.7457\n",
            "Epoch 87/120\n",
            "1250/1250 [==============================] - 60s 48ms/step - loss: 0.7361 - acc: 0.7704 - val_loss: 0.8011 - val_acc: 0.7560\n",
            "Epoch 88/120\n",
            "1250/1250 [==============================] - 57s 46ms/step - loss: 0.7330 - acc: 0.7733 - val_loss: 0.8674 - val_acc: 0.7391\n",
            "Epoch 89/120\n",
            "1250/1250 [==============================] - 55s 44ms/step - loss: 0.7326 - acc: 0.7716 - val_loss: 0.8183 - val_acc: 0.7539\n",
            "Epoch 90/120\n",
            "1250/1250 [==============================] - 56s 45ms/step - loss: 0.7367 - acc: 0.7708 - val_loss: 0.8544 - val_acc: 0.7422\n",
            "Epoch 91/120\n",
            "1250/1250 [==============================] - 58s 46ms/step - loss: 0.7232 - acc: 0.7752 - val_loss: 0.8165 - val_acc: 0.7515\n",
            "Epoch 92/120\n",
            "1250/1250 [==============================] - 58s 46ms/step - loss: 0.7294 - acc: 0.7725 - val_loss: 0.8422 - val_acc: 0.7448\n",
            "Epoch 93/120\n",
            "1250/1250 [==============================] - 56s 45ms/step - loss: 0.7280 - acc: 0.7741 - val_loss: 0.8325 - val_acc: 0.7477\n",
            "Epoch 94/120\n",
            "1250/1250 [==============================] - 57s 46ms/step - loss: 0.7247 - acc: 0.7748 - val_loss: 0.7964 - val_acc: 0.7597\n",
            "Epoch 95/120\n",
            "1250/1250 [==============================] - 60s 48ms/step - loss: 0.7194 - acc: 0.7755 - val_loss: 0.8287 - val_acc: 0.7504\n",
            "Epoch 96/120\n",
            "1250/1250 [==============================] - 57s 46ms/step - loss: 0.7263 - acc: 0.7746 - val_loss: 0.8126 - val_acc: 0.7550\n",
            "Epoch 97/120\n",
            "1250/1250 [==============================] - 60s 48ms/step - loss: 0.7241 - acc: 0.7739 - val_loss: 0.8480 - val_acc: 0.7437\n",
            "Epoch 98/120\n",
            "1250/1250 [==============================] - 56s 45ms/step - loss: 0.7212 - acc: 0.7761 - val_loss: 0.8012 - val_acc: 0.7594\n",
            "Epoch 99/120\n",
            "1250/1250 [==============================] - 58s 47ms/step - loss: 0.7143 - acc: 0.7790 - val_loss: 0.8438 - val_acc: 0.7477\n",
            "Epoch 100/120\n",
            "1250/1250 [==============================] - 58s 46ms/step - loss: 0.7145 - acc: 0.7751 - val_loss: 0.7910 - val_acc: 0.7601\n",
            "Epoch 101/120\n",
            "1250/1250 [==============================] - 56s 45ms/step - loss: 0.7110 - acc: 0.7790 - val_loss: 0.8946 - val_acc: 0.7339\n",
            "Epoch 102/120\n",
            "1250/1250 [==============================] - 59s 47ms/step - loss: 0.7136 - acc: 0.7793 - val_loss: 0.8450 - val_acc: 0.7460\n",
            "Epoch 103/120\n",
            "1250/1250 [==============================] - 58s 46ms/step - loss: 0.7122 - acc: 0.7792 - val_loss: 0.7901 - val_acc: 0.7609\n",
            "Epoch 104/120\n",
            "1250/1250 [==============================] - 58s 46ms/step - loss: 0.7141 - acc: 0.7754 - val_loss: 0.8174 - val_acc: 0.7539\n",
            "Epoch 105/120\n",
            "1250/1250 [==============================] - 60s 48ms/step - loss: 0.7063 - acc: 0.7820 - val_loss: 0.7858 - val_acc: 0.7630\n",
            "Epoch 106/120\n",
            "1250/1250 [==============================] - 57s 45ms/step - loss: 0.7110 - acc: 0.7777 - val_loss: 0.8091 - val_acc: 0.7578\n",
            "Epoch 107/120\n",
            "1250/1250 [==============================] - 56s 44ms/step - loss: 0.7038 - acc: 0.7805 - val_loss: 0.8053 - val_acc: 0.7602\n",
            "Epoch 108/120\n",
            "1250/1250 [==============================] - 55s 44ms/step - loss: 0.7037 - acc: 0.7821 - val_loss: 0.8156 - val_acc: 0.7570\n",
            "Epoch 109/120\n",
            "1250/1250 [==============================] - 58s 46ms/step - loss: 0.7023 - acc: 0.7822 - val_loss: 0.8531 - val_acc: 0.7473\n",
            "Epoch 110/120\n",
            "1250/1250 [==============================] - 59s 48ms/step - loss: 0.7045 - acc: 0.7811 - val_loss: 0.7997 - val_acc: 0.7572\n",
            "Epoch 111/120\n",
            "1250/1250 [==============================] - 58s 47ms/step - loss: 0.6962 - acc: 0.7831 - val_loss: 0.8066 - val_acc: 0.7597\n",
            "Epoch 112/120\n",
            "1250/1250 [==============================] - 60s 48ms/step - loss: 0.6935 - acc: 0.7850 - val_loss: 0.8067 - val_acc: 0.7599\n",
            "Epoch 113/120\n",
            "1250/1250 [==============================] - 58s 47ms/step - loss: 0.6955 - acc: 0.7838 - val_loss: 0.8503 - val_acc: 0.7448\n",
            "Epoch 114/120\n",
            "1250/1250 [==============================] - 59s 47ms/step - loss: 0.6951 - acc: 0.7844 - val_loss: 0.8318 - val_acc: 0.7520\n",
            "Epoch 115/120\n",
            "1250/1250 [==============================] - 57s 46ms/step - loss: 0.6926 - acc: 0.7850 - val_loss: 0.8314 - val_acc: 0.7543\n",
            "Epoch 116/120\n",
            "1250/1250 [==============================] - 56s 45ms/step - loss: 0.6946 - acc: 0.7857 - val_loss: 0.8385 - val_acc: 0.7509\n",
            "Epoch 117/120\n",
            "1250/1250 [==============================] - 57s 46ms/step - loss: 0.6932 - acc: 0.7847 - val_loss: 0.8096 - val_acc: 0.7577\n",
            "Epoch 118/120\n",
            "1250/1250 [==============================] - 54s 44ms/step - loss: 0.6872 - acc: 0.7874 - val_loss: 0.8228 - val_acc: 0.7535\n",
            "Epoch 119/120\n",
            "1250/1250 [==============================] - 59s 48ms/step - loss: 0.6862 - acc: 0.7888 - val_loss: 0.7776 - val_acc: 0.7650\n",
            "Epoch 120/120\n",
            "1250/1250 [==============================] - 60s 48ms/step - loss: 0.6867 - acc: 0.7875 - val_loss: 0.7954 - val_acc: 0.7638\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fdb80518ef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "CrohKh6m6Mc_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b7d7f444-2f67-4d0f-dadc-77d35e67ab15"
      },
      "cell_type": "code",
      "source": [
        "## Score trained model.\n",
        "scores = model3.evaluate(X_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 2s 228us/step\n",
            "Test loss: 0.809547947216034\n",
            "Test accuracy: 0.7635\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gcNDQjLS8PJe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Moel4**  Dropout with Data augmentation"
      ]
    },
    {
      "metadata": {
        "id": "gujUtnCN8bMm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model4 = Sequential()\n",
        "model4.add(Conv2D(32, (3,3), strides = (1,1), padding='same', activation ='relu', input_shape =(32,32,3),kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(0.0001)))\n",
        "model4.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "model4.add(Conv2D(48, (3,3), strides = (1,1), padding='same', activation='relu', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(0.0001)))\n",
        "model4.add(Dropout(0.25))\n",
        "\n",
        "model4.add(Conv2D(64, (3,3), strides= (1,1),padding='same', activation='relu', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(0.0001)))\n",
        "model4.add(MaxPooling2D(pool_size=(2,2), padding= 'same'))\n",
        "model4.add(Dropout(0.5))\n",
        " \n",
        "model4.add(Conv2D(128, (3,3), strides= (2,2), padding='same', activation = 'relu',kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(0.0001)))\n",
        "model4.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
        "model4.add(Dropout(0.25))\n",
        "\n",
        "model4.add(Conv2D(256, (3,3), strides= (2,2), padding='same', activation = 'relu', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(0.0001)))\n",
        "model4.add(Dropout(0.1))\n",
        "\n",
        "model4.add(Flatten())\n",
        "model4.add(Dense(128, activation='relu',  kernel_regularizer= regularizers.l2(0.0001)))\n",
        "model4.add(Dense(64, activation= 'relu'))\n",
        "model4.add(Dense(10, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gIDbTrKz95kJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4114
        },
        "outputId": "8c374da7-06e9-4196-94fd-dde10f58eb7f"
      },
      "cell_type": "code",
      "source": [
        "model4.compile(loss='categorical_crossentropy', \n",
        "               optimizer='adagrad', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "## Train the model and get the training and validation loss\n",
        "model4.fit_generator(datagen.flow(X_train, y_train, batch_size=40),\n",
        "                    steps_per_epoch=train_images.shape[0]/ 40,epochs= 120,\n",
        "                    verbose=1,validation_data=(X_val,y_val))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/120\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 2.0188 - acc: 0.2863 - val_loss: 1.7681 - val_acc: 0.3966\n",
            "Epoch 2/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.7864 - acc: 0.3767 - val_loss: 1.6368 - val_acc: 0.4357\n",
            "Epoch 3/120\n",
            "1250/1250 [==============================] - 48s 39ms/step - loss: 1.6795 - acc: 0.4195 - val_loss: 1.5158 - val_acc: 0.4893\n",
            "Epoch 4/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.6070 - acc: 0.4438 - val_loss: 1.4721 - val_acc: 0.4980\n",
            "Epoch 5/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.5546 - acc: 0.4660 - val_loss: 1.3900 - val_acc: 0.5249\n",
            "Epoch 6/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.5097 - acc: 0.4840 - val_loss: 1.3863 - val_acc: 0.5308\n",
            "Epoch 7/120\n",
            "1250/1250 [==============================] - 50s 40ms/step - loss: 1.4816 - acc: 0.4947 - val_loss: 1.4001 - val_acc: 0.5272\n",
            "Epoch 8/120\n",
            "1250/1250 [==============================] - 50s 40ms/step - loss: 1.4488 - acc: 0.5045 - val_loss: 1.3525 - val_acc: 0.5316\n",
            "Epoch 9/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.4245 - acc: 0.5142 - val_loss: 1.2672 - val_acc: 0.5745\n",
            "Epoch 10/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.4000 - acc: 0.5239 - val_loss: 1.2770 - val_acc: 0.5736\n",
            "Epoch 11/120\n",
            "1250/1250 [==============================] - 49s 40ms/step - loss: 1.3739 - acc: 0.5312 - val_loss: 1.2398 - val_acc: 0.5813\n",
            "Epoch 12/120\n",
            "1250/1250 [==============================] - 49s 40ms/step - loss: 1.3653 - acc: 0.5366 - val_loss: 1.2177 - val_acc: 0.5878\n",
            "Epoch 13/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.3367 - acc: 0.5452 - val_loss: 1.2293 - val_acc: 0.5833\n",
            "Epoch 14/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.3246 - acc: 0.5528 - val_loss: 1.1795 - val_acc: 0.6060\n",
            "Epoch 15/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.3076 - acc: 0.5562 - val_loss: 1.1687 - val_acc: 0.6128\n",
            "Epoch 16/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.2952 - acc: 0.5627 - val_loss: 1.1784 - val_acc: 0.6063\n",
            "Epoch 17/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.2844 - acc: 0.5666 - val_loss: 1.1998 - val_acc: 0.6002\n",
            "Epoch 18/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.2696 - acc: 0.5712 - val_loss: 1.1478 - val_acc: 0.6211\n",
            "Epoch 19/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.2631 - acc: 0.5741 - val_loss: 1.1294 - val_acc: 0.6260\n",
            "Epoch 20/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.2504 - acc: 0.5809 - val_loss: 1.1423 - val_acc: 0.6191\n",
            "Epoch 21/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.2426 - acc: 0.5846 - val_loss: 1.1072 - val_acc: 0.6336\n",
            "Epoch 22/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.2334 - acc: 0.5841 - val_loss: 1.0860 - val_acc: 0.6435\n",
            "Epoch 23/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.2251 - acc: 0.5896 - val_loss: 1.1278 - val_acc: 0.6330\n",
            "Epoch 24/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.2201 - acc: 0.5903 - val_loss: 1.0854 - val_acc: 0.6448\n",
            "Epoch 25/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.2065 - acc: 0.5955 - val_loss: 1.0608 - val_acc: 0.6525\n",
            "Epoch 26/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.2000 - acc: 0.5987 - val_loss: 1.0643 - val_acc: 0.6528\n",
            "Epoch 27/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.1921 - acc: 0.6003 - val_loss: 1.0880 - val_acc: 0.6437\n",
            "Epoch 28/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.1915 - acc: 0.6020 - val_loss: 1.0552 - val_acc: 0.6590\n",
            "Epoch 29/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.1843 - acc: 0.6064 - val_loss: 1.0705 - val_acc: 0.6511\n",
            "Epoch 30/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.1785 - acc: 0.6070 - val_loss: 1.0462 - val_acc: 0.6622\n",
            "Epoch 31/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.1810 - acc: 0.6056 - val_loss: 1.0323 - val_acc: 0.6665\n",
            "Epoch 32/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.1691 - acc: 0.6113 - val_loss: 1.1017 - val_acc: 0.6410\n",
            "Epoch 33/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.1684 - acc: 0.6110 - val_loss: 1.0430 - val_acc: 0.6615\n",
            "Epoch 34/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.1537 - acc: 0.6182 - val_loss: 1.0632 - val_acc: 0.6516\n",
            "Epoch 35/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.1607 - acc: 0.6160 - val_loss: 1.0169 - val_acc: 0.6678\n",
            "Epoch 36/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.1537 - acc: 0.6187 - val_loss: 1.0088 - val_acc: 0.6736\n",
            "Epoch 37/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.1455 - acc: 0.6223 - val_loss: 1.0786 - val_acc: 0.6484\n",
            "Epoch 38/120\n",
            "1250/1250 [==============================] - 48s 39ms/step - loss: 1.1445 - acc: 0.6227 - val_loss: 1.0268 - val_acc: 0.6693\n",
            "Epoch 39/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.1385 - acc: 0.6210 - val_loss: 1.0529 - val_acc: 0.6563\n",
            "Epoch 40/120\n",
            "1250/1250 [==============================] - 48s 39ms/step - loss: 1.1361 - acc: 0.6236 - val_loss: 1.0450 - val_acc: 0.6605\n",
            "Epoch 41/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.1295 - acc: 0.6262 - val_loss: 1.0022 - val_acc: 0.6754\n",
            "Epoch 42/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.1218 - acc: 0.6292 - val_loss: 1.0144 - val_acc: 0.6705\n",
            "Epoch 43/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.1270 - acc: 0.6273 - val_loss: 1.0189 - val_acc: 0.6689\n",
            "Epoch 44/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.1169 - acc: 0.6287 - val_loss: 0.9850 - val_acc: 0.6804\n",
            "Epoch 45/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.1153 - acc: 0.6327 - val_loss: 0.9914 - val_acc: 0.6799\n",
            "Epoch 46/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.1137 - acc: 0.6316 - val_loss: 1.0064 - val_acc: 0.6733\n",
            "Epoch 47/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.1138 - acc: 0.6293 - val_loss: 1.0216 - val_acc: 0.6668\n",
            "Epoch 48/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.1110 - acc: 0.6316 - val_loss: 1.0240 - val_acc: 0.6639\n",
            "Epoch 49/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.1053 - acc: 0.6366 - val_loss: 1.0276 - val_acc: 0.6654\n",
            "Epoch 50/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.1097 - acc: 0.6357 - val_loss: 0.9874 - val_acc: 0.6803\n",
            "Epoch 51/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.1019 - acc: 0.6386 - val_loss: 0.9933 - val_acc: 0.6775\n",
            "Epoch 52/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0940 - acc: 0.6422 - val_loss: 0.9934 - val_acc: 0.6774\n",
            "Epoch 53/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0897 - acc: 0.6421 - val_loss: 0.9945 - val_acc: 0.6770\n",
            "Epoch 54/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0875 - acc: 0.6412 - val_loss: 1.0083 - val_acc: 0.6702\n",
            "Epoch 55/120\n",
            "1250/1250 [==============================] - 48s 39ms/step - loss: 1.0929 - acc: 0.6433 - val_loss: 0.9820 - val_acc: 0.6834\n",
            "Epoch 56/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0864 - acc: 0.6434 - val_loss: 0.9900 - val_acc: 0.6808\n",
            "Epoch 57/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0811 - acc: 0.6463 - val_loss: 0.9656 - val_acc: 0.6845\n",
            "Epoch 58/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0797 - acc: 0.6443 - val_loss: 0.9646 - val_acc: 0.6870\n",
            "Epoch 59/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0784 - acc: 0.6441 - val_loss: 0.9491 - val_acc: 0.6915\n",
            "Epoch 60/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0810 - acc: 0.6470 - val_loss: 1.0094 - val_acc: 0.6750\n",
            "Epoch 61/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0691 - acc: 0.6474 - val_loss: 0.9572 - val_acc: 0.6919\n",
            "Epoch 62/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0682 - acc: 0.6489 - val_loss: 0.9450 - val_acc: 0.6907\n",
            "Epoch 63/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0717 - acc: 0.6463 - val_loss: 0.9716 - val_acc: 0.6841\n",
            "Epoch 64/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0598 - acc: 0.6541 - val_loss: 0.9507 - val_acc: 0.6915\n",
            "Epoch 65/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0666 - acc: 0.6492 - val_loss: 0.9911 - val_acc: 0.6797\n",
            "Epoch 66/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0606 - acc: 0.6562 - val_loss: 0.9815 - val_acc: 0.6824\n",
            "Epoch 67/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0539 - acc: 0.6551 - val_loss: 0.9308 - val_acc: 0.6998\n",
            "Epoch 68/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0591 - acc: 0.6524 - val_loss: 0.9859 - val_acc: 0.6809\n",
            "Epoch 69/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0525 - acc: 0.6560 - val_loss: 0.9473 - val_acc: 0.6917\n",
            "Epoch 70/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0497 - acc: 0.6562 - val_loss: 0.9277 - val_acc: 0.6970\n",
            "Epoch 71/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0500 - acc: 0.6556 - val_loss: 0.9429 - val_acc: 0.6937\n",
            "Epoch 72/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0478 - acc: 0.6572 - val_loss: 0.9760 - val_acc: 0.6844\n",
            "Epoch 73/120\n",
            "1250/1250 [==============================] - 50s 40ms/step - loss: 1.0455 - acc: 0.6565 - val_loss: 0.9579 - val_acc: 0.6896\n",
            "Epoch 74/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0493 - acc: 0.6556 - val_loss: 0.9444 - val_acc: 0.6947\n",
            "Epoch 75/120\n",
            "1250/1250 [==============================] - 49s 40ms/step - loss: 1.0417 - acc: 0.6603 - val_loss: 0.9687 - val_acc: 0.6863\n",
            "Epoch 76/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0397 - acc: 0.6607 - val_loss: 0.9500 - val_acc: 0.6921\n",
            "Epoch 77/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0363 - acc: 0.6606 - val_loss: 0.9341 - val_acc: 0.6980\n",
            "Epoch 78/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0407 - acc: 0.6619 - val_loss: 0.9487 - val_acc: 0.6935\n",
            "Epoch 79/120\n",
            "1250/1250 [==============================] - 49s 40ms/step - loss: 1.0336 - acc: 0.6616 - val_loss: 0.9220 - val_acc: 0.7010\n",
            "Epoch 80/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0348 - acc: 0.6617 - val_loss: 0.9473 - val_acc: 0.6935\n",
            "Epoch 81/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0341 - acc: 0.6638 - val_loss: 0.9283 - val_acc: 0.7008\n",
            "Epoch 82/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0381 - acc: 0.6623 - val_loss: 0.9203 - val_acc: 0.7028\n",
            "Epoch 83/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0268 - acc: 0.6630 - val_loss: 0.9497 - val_acc: 0.6942\n",
            "Epoch 84/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0239 - acc: 0.6649 - val_loss: 0.9033 - val_acc: 0.7101\n",
            "Epoch 85/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0267 - acc: 0.6649 - val_loss: 0.9626 - val_acc: 0.6915\n",
            "Epoch 86/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0216 - acc: 0.6652 - val_loss: 0.9501 - val_acc: 0.6945\n",
            "Epoch 87/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0211 - acc: 0.6678 - val_loss: 0.9530 - val_acc: 0.6915\n",
            "Epoch 88/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0223 - acc: 0.6657 - val_loss: 0.9059 - val_acc: 0.7085\n",
            "Epoch 89/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0236 - acc: 0.6669 - val_loss: 0.9418 - val_acc: 0.6950\n",
            "Epoch 90/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0232 - acc: 0.6656 - val_loss: 0.9181 - val_acc: 0.7051\n",
            "Epoch 91/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0172 - acc: 0.6687 - val_loss: 0.9038 - val_acc: 0.7058\n",
            "Epoch 92/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0166 - acc: 0.6700 - val_loss: 0.9036 - val_acc: 0.7071\n",
            "Epoch 93/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0156 - acc: 0.6668 - val_loss: 0.9120 - val_acc: 0.7052\n",
            "Epoch 94/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0102 - acc: 0.6729 - val_loss: 0.9312 - val_acc: 0.7006\n",
            "Epoch 95/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0187 - acc: 0.6670 - val_loss: 0.9226 - val_acc: 0.7023\n",
            "Epoch 96/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0082 - acc: 0.6727 - val_loss: 0.9412 - val_acc: 0.6975\n",
            "Epoch 97/120\n",
            "1250/1250 [==============================] - 48s 39ms/step - loss: 1.0077 - acc: 0.6707 - val_loss: 0.9138 - val_acc: 0.7045\n",
            "Epoch 98/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0091 - acc: 0.6707 - val_loss: 0.9533 - val_acc: 0.6943\n",
            "Epoch 99/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0144 - acc: 0.6704 - val_loss: 0.9107 - val_acc: 0.7064\n",
            "Epoch 100/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0039 - acc: 0.6726 - val_loss: 0.9118 - val_acc: 0.7078\n",
            "Epoch 101/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 0.9989 - acc: 0.6751 - val_loss: 0.8953 - val_acc: 0.7123\n",
            "Epoch 102/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0034 - acc: 0.6747 - val_loss: 0.9241 - val_acc: 0.7016\n",
            "Epoch 103/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0042 - acc: 0.6738 - val_loss: 0.8984 - val_acc: 0.7144\n",
            "Epoch 104/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 1.0020 - acc: 0.6748 - val_loss: 0.9227 - val_acc: 0.7028\n",
            "Epoch 105/120\n",
            "1250/1250 [==============================] - 48s 39ms/step - loss: 0.9971 - acc: 0.6743 - val_loss: 0.9220 - val_acc: 0.7037\n",
            "Epoch 106/120\n",
            "1250/1250 [==============================] - 48s 39ms/step - loss: 0.9993 - acc: 0.6738 - val_loss: 0.9185 - val_acc: 0.7046\n",
            "Epoch 107/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 0.9945 - acc: 0.6762 - val_loss: 0.8886 - val_acc: 0.7169\n",
            "Epoch 108/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 0.9937 - acc: 0.6773 - val_loss: 0.9266 - val_acc: 0.7052\n",
            "Epoch 109/120\n",
            "1250/1250 [==============================] - 48s 39ms/step - loss: 0.9958 - acc: 0.6775 - val_loss: 0.8945 - val_acc: 0.7154\n",
            "Epoch 110/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 0.9917 - acc: 0.6791 - val_loss: 0.9222 - val_acc: 0.7035\n",
            "Epoch 111/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 0.9860 - acc: 0.6798 - val_loss: 0.9087 - val_acc: 0.7122\n",
            "Epoch 112/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 0.9880 - acc: 0.6800 - val_loss: 0.9132 - val_acc: 0.7083\n",
            "Epoch 113/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 0.9899 - acc: 0.6789 - val_loss: 0.8827 - val_acc: 0.7199\n",
            "Epoch 114/120\n",
            "1250/1250 [==============================] - 48s 39ms/step - loss: 0.9851 - acc: 0.6792 - val_loss: 0.9043 - val_acc: 0.7121\n",
            "Epoch 115/120\n",
            "1250/1250 [==============================] - 48s 39ms/step - loss: 0.9850 - acc: 0.6786 - val_loss: 0.9502 - val_acc: 0.6971\n",
            "Epoch 116/120\n",
            "1250/1250 [==============================] - 48s 39ms/step - loss: 0.9852 - acc: 0.6794 - val_loss: 0.8873 - val_acc: 0.7150\n",
            "Epoch 117/120\n",
            "1250/1250 [==============================] - 48s 39ms/step - loss: 0.9819 - acc: 0.6812 - val_loss: 0.8814 - val_acc: 0.7194\n",
            "Epoch 118/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 0.9782 - acc: 0.6821 - val_loss: 0.9054 - val_acc: 0.7115\n",
            "Epoch 119/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 0.9794 - acc: 0.6828 - val_loss: 0.8955 - val_acc: 0.7172\n",
            "Epoch 120/120\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 0.9803 - acc: 0.6807 - val_loss: 0.9100 - val_acc: 0.7109\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fdb6cc00cf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "HiyA3NfD_Dsw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For k-fold validation, I have used the best model from above"
      ]
    },
    {
      "metadata": {
        "id": "p0J9TzMT_GFb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "faf5d3fa-f41c-4c22-84aa-18e7f70fd242"
      },
      "cell_type": "code",
      "source": [
        "## Get the training dataset again\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "X_train = train_images.reshape(train_images.shape[0], 32, 32, 3)\n",
        "y_train_Kfold = np_utils.to_categorical(train_labels, 10)\n",
        "X_train_Kfold = (X_train/255).astype('float32')\n",
        "\n",
        "\n",
        "scores = []\n",
        "cv = KFold(n_splits=5, random_state=42, shuffle=False)\n",
        "for train_index, test_index in cv.split(X_train_Kfold):\n",
        "  \n",
        "    ## K-fold Split of the dataset(where K=20, 19 of the folds are for training and 1 of them is for validating)\n",
        "    X_train, X_val, y_train, y_val = X_train_Kfold[train_index], X_train_Kfold[test_index], y_train_Kfold[train_index], y_train_Kfold[test_index]\n",
        "\n",
        "    ## Compile the optimizer and train \n",
        "    model2.compile(loss='categorical_crossentropy', optimizer='adagrad', metrics=['accuracy'])\n",
        "    model2.fit(X_train, y_train)\n",
        "\n",
        "    ## Get the score on the test dataset\n",
        "    scores.append(model2.evaluate(X_test, y_test))\n",
        "    \n",
        "print(\"K-fold cross validation DONE...\")\n",
        "print(\"The Cross validation Score [Accuracy]\",np.mean(scores[1]))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "40000/40000 [==============================] - 38s 954us/step - loss: 14.5698 - acc: 0.1000\n",
            "10000/10000 [==============================] - 3s 305us/step\n",
            "Epoch 1/1\n",
            "40000/40000 [==============================] - 30s 749us/step - loss: 1.4803 - acc: 0.4821\n",
            "10000/10000 [==============================] - 2s 221us/step\n",
            "Epoch 1/1\n",
            "40000/40000 [==============================] - 30s 754us/step - loss: 1.1886 - acc: 0.5946\n",
            "10000/10000 [==============================] - 2s 231us/step\n",
            "Epoch 1/1\n",
            "40000/40000 [==============================] - 30s 760us/step - loss: 1.0761 - acc: 0.6472\n",
            "10000/10000 [==============================] - 2s 236us/step\n",
            "Epoch 1/1\n",
            "40000/40000 [==============================] - 31s 778us/step - loss: 8.7303 - acc: 0.3230\n",
            "10000/10000 [==============================] - 3s 250us/step\n",
            "K-fold cross validation DONE...\n",
            "The Cross validation Score [Accuracy] 0.8608836936950683\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "W_tmzg3XCOam",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}